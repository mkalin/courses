     1	Sections 6.2/6.3/6.4 in text
     2	
     3	Typical caches:
     4	
     5	CPU registers:        4-byte words (0 latency)
     6	TLB:                  address translations (32-bits per address) (0 latency)
     7	L1 cache:             32-byte blocks (0-2 latency)
     8	
     9	L2 cache:             32-byte blocks (2 - 10 latency)
    10	
    11	Virtual memory:       4-KB page (100 latency in RAM)
    12	Buffer cache:         Parts of files (100 latency in RAM) 
    13	
    14	Network buffer cache: Parts of files (disk)        (10,000,000 latency = 10M)
    15	Browser cache:        Web pages (disk, client)
    16	Web cache:            Web pages (disk, remote server) (1,000,000,000 = 1G)
    17	 
    18	
    19	L1 and L2 issues: expensive SRAM and more transitors per bit stored
    20	
    21	Program Locality: temporal (reusing pre-cached objects) and spatial (cost of
    22	                  fetching a block amortized across subsequent accesses to that
    23	                  block).
    24	
    25	Cache Misses
    26	
    27	   -- cold or compulsory miss: cache is empty WRT process P so that P's
    28	      instruction/data must be loaded into the cache
    29	
    30	   -- conflict miss: blocks from memory level K + 1 (e.g., RAM) 
    31	      map to blocks at memory level K (e.g., L1), sizeof(K) < sizeof(K + 1).
    32	      For example, RAM blocks 0, 4, 8, and 12 map to cache block 0.
    33	      Now suppose that process P requires blocks in this order:
    34	 
    35	               0,8,0,8,...,08
    36	
    37	      RAM block 0 and 8 both map to cache block 0; hence, the loading of
    38	      each RAM block causes a subsequent "conflict miss"
    39	
    40	   -- capacity miss: a process typically uses a "working set" of cache
    41	      blocks (e.g., blocks b1, b2, b3, and b4). But the cache cannot hold
    42	      the entire working set; that is, 
    43	      sizeof(working set) > sizeof(allocated cache).
    44	
    45	
    46	
    47	
    48	
    49	
    50	
    51	
    52	
    53	
    54	
    55	
    56	
    57	
    58	
    59	
    60	
    61	
    62	Cache structure:
    63	
    64	  Cache is partitioned into sets. If just one set (fully associative), if more 
    65	  than one set (set-associative).
    66	
    67	  N = number of sets
    68	
    69	  A cache set is divided into lines, the analogs of memory pages. Each line 
    70	  includes a block of bytes. Each set has the same number of blocks.
    71	
    72	  K = lines per set
    73	
    74	  Each line has a certain number of words (bytes).
    75	
    76	  L = bytes per line
    77	
    78	     Example:   N = 256
    79	                K = 4
    80	                L = 64
    81	
    82	                Capacity = N * K * L = 32,768 bytes
    83	
    84	  The larger K, the more overhead bits == the larger the directory
    85	
    86	  Definitions:
    87	
    88	     N = 1           ==> fully associative
    89	     N > 1 and K = 1 ==> direct mapped
    90	     N > 1 and K > 1 ==> K-way set-associative
    91	
    92	  In modern designs, TLB is often fully associative, L1s tend to be 
    93	  K-way set-associative, and L2 tend to be set-associative or direct-mapped.
    94	  
    95	
    96	  "Generic" is set-associative:
    97	
    98	     Multiple sets, each set consists of multiple lines:
    99	     Each line:
   100	               +------+-----------+--------------------------------------+
   101	               | vbit | tag = key |  block of bytes                      |
   102	               +------+-----------+--------------------------------------+
   103	
   104	     The m address bits in the cache address are partitioned into
   105	
   106	            -- set index bits
   107	            -- tag bits
   108	            -- block offset
   109	
   110	     The vbit + tag bits = "overhead bits"
   111	     The block of bytes  = "data bits"
   112	
   113	     Cache capacity ignores overhead bits, e.g., a 256K cache = 256K of blocks.
   114	
   115	
   116	
   117	
   118	
   119	
   120	
   121	
   122	Definitions:
   123	
   124	  The collection of all tag bits is called the cache directory.
   125	
   126	  ** Direct-Mapped Cache: 1 line per set  (simplest to design/implement)
   127	     
   128	     Address format for DM cache:
   129	
   130	                  +-----+-----------+-------------+
   131	                  | tag | set index | byte offset |  
   132	                  +-----+-----------+-------------+
   133	                          middle bits so that contiguous blocks in memory
   134	                          don't map to the same cache block
   135	
   136	     A direct-mapped cache is just an array with a set index
   137	
   138	
   139	  ** Set-Associative Cache: > 1 lines per set (E)
   140	                            4, 8,...,128,256
   141	             
   142	     Replacement policy: (doesn't arise in DM) LRU, LFU, or random
   143	     
   144	     A set-associative cache is an associative memory (key/value pairs)
   145	
   146	  ** Fully Associative: 1 set 
   147	
   148	     Each line's tag must be searched in fully associative.
   149	     Used only in TLBs, which tend to be small.
   150	
   151	  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
   152	
   153	  Definitions: A cache is "write through" if a write to the cache also writes
   154	               back to main memory. 
   155	
   156	               If a write to the cache does not automatically trigger a write
   157	               to RAM, then the write to RAM occurs only when required (e.g.,
   158	               another processes needs to access the RAM location); at this
   159	               point, the cache contents are written back.
   160	
   161	  Write issues: write through versus write back (dirty bit)
   162	                no-write-allocate ==> write only to RAM, not to the cache
   163	                    
   164	                write-through are typically no-write-allocate
   165	                write-back are typically write-allocate
   166	
   167	  i-cache, d-cache, unified cache
   168	
   169	  Typically, L1 has an i-cache and a d-cache
   170	             L2 has a unified cache
   171	
   172	  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
   173	
   174	
   175	
   176	
   177	
   178	
   179	
   180	
   181	
   182	  Metrics:
   183	
   184	      -- miss rate
   185	      -- hit rate
   186	      -- hit time
   187	      -- miss penalty
   188	
   189	  Cache design and metrics:
   190	
   191	    -- the larger the cache, the higher the hit rate
   192	    -- the larger the cache, the higher the hit time (more searching)
   193	
   194	    -- the larger the block size, the more potential for locality
   195	    -- the larger the block size, the smaller the sets or the lines per set
   196	 
   197	    -- the higher the associativity, the lower the conflict misses
   198	    -- the higher the associativity, the higher the ratio of tag to data bits
   199	     
   200	
   201	  Classic space/time tradeoff: Enhancing the hit rate may require large
   202	                               associativity, that is, large directory.
   203	
   204	
   205	  ****************************
   206	
   207	  Cache addressing: most processors use virtual addresses but some 
   208	                    (e.g., the Alpha) use physical addresses
   209	
   210	
   211	  Example with a 32-bit virtual address:
   212	
   213	     512 sets, 4 lines per set, and 128 bytes per line
   214	
   215	     log(512)   =  9 bits for the set
   216	     log(128)   =  7 bits for the word offset
   217	     32 - 9 - 7 = 16 bits for the tag
   218	
   219	        16 bits          9 bits   7 bits
   220	     +----------------+---------+-------+
   221	     |   tag          |  set    | byte  |
   222	     +----------------+---------+-------+      
   223	
   224	  
   225	
   226	
   227	
   228	
   229	
   230	
   231	
   232	
   233	
   234	
   235	
   236	
   237	
   238	
   239	
   240	
   241	
   242	  Example of DM versus K-way SA:
   243	
   244	     C1 is K-way set associative: 512 sets, 8 lines per set, 2 words per line
   245	                                  (Assume a 32-bit word)
   246	
   247	     C1's capacity is 512 * 8 * 32 = 262,144 bits
   248	        -- 9 bits for the set ID
   249	        -- 1 bit for the word offset
   250	        -- 22 bits for the tag
   251	
   252	        -- directory size = 512 * 8 * 22 = 90,112 bits (34% of all bits)
   253	
   254	     
   255	     C2 is direct mapped: 4,096 sets, 1 line per set, 2 words per line
   256	                          (Again assume a 32-bit word)
   257	
   258	     C2's capacity i9s 4096 * 1 * 32 = 262,144 bits
   259	        -- 12 bits for the set ID 
   260	        -- 1 bit for the word offset
   261	        -- 19 bits for the tag
   262	
   263	        -- directory size = 4096 * 19 = 77,824 (30% of all bits)
   264	
