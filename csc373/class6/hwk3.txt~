
                                CSC373: Computer Systems 1
                                     Summer 1, 2007
                                        Homework 3

Sections 6.2 and especially 6.3 and 6.4 in the book cover this material.

Points: 100
Due:    Before midnight, July 11

Terminology: Suppose that process P is executing instructions such as

               ADD RAM[1], RAM[2], RAM[100]    ;; RAM[100] = RAM[1] + RAM[2]

For this instruction, P generates the (virtual) addresses

                       1, 2, 100

for the operands RAM[1], RAM[2], and RAM[100]. Note that the ADD instruction itself
is located originally in memory and must be fetched to the CPU; so the address of 
this instruction also must be generated by process P. 

Now we generalize. A string of memory references such as

                      1, 4, 8, 5,...,9, 17

is called a "reference string" and indicates the memory addresses that an executing 
process generates. The string is read from left to right: 1 is the first address, 
4 is the second, and so on. For purposes of this exercise, we don't care whether  
the addresses are of (a) data or (b) instructions; we care only about the reference 
string and its impact upon a cache. For example, there's
a reference to address 17 at the end. 

The L1 cache will be searched to see if the word with (virtual) address 17 occurs 
in the cache. In the questions below, a virtual address-to-cache address 
translation scheme will be given.

For simplicity, we assume that every memory word (data or instructions) is 8 bits
and that addresses are of words (that is, 8-bit bytes). The addresses are 32 bits.

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
Please answer each of the following and show your reasoning.

 1. Cache C has 16 1-word blocks and is direct mapped. Process P 
    generates this reference string, each a word address:

	1, 4, 8, 5, 20, 17, 19, 56, 9, 11, 4, 43, 5, 6, 9, 17, 9, 56, 9

    C is initially empty. For each memory reference above, label it as a
    Hit or a Miss. Show C's contents after the last memory reference.
    Assume that a word's cache address is its memory address modulo the
    cache's size (for example, the word at address 1 has cache address 1 % 16 = 1;
    the word at address 19 has cache address 19 % 16 = 3; etc.)

 2. Use the same reference string and assumptions as in question (1). This time 
    assume that cache C is still direct mapped but with four 4-word blocks. So
    C's capacity remains unchanged but its structure differs. Do the same as 
    in (1), that is, label each word reference as a cache Hit or Miss and show the 
    cache's contents at the end. To begin, we assume the cache is empty.

 3. Use the same reference string and assumptions as in question (1). Assume 
    that cache C is 2-way set associative with 8 sets. Each block holds 1 word. 
    Replacement is LRU. A memory reference's cache set is the memory address 
    modulus 8 (for example, the word at address 8 goes into set 0; the word at 
    address 11 goes into set 3; etc.) Do the same as in (1).

 4. Computer system S uses 32-bit virtual addresses as cache addresses. The cache 
    address has three fields, left to right:

                tag bits    set identifier    word offset

    So how many bits are used in each field given 1,024 sets each with 8 lines. 
    Each line contains 32 8-bit words.

 5. A cache's total bit size is partitioned into "data bits" (that is, data or 
    instructions) and "overhead bits" (bits for directory tags, the valid bit, 
    the LRU bits, if used, and so on). For now, the only "overhead bits" of 
    interest are the directory or tag bits. Consider two set-associative caches 
    with the same capacity:

	C1: 1024 sets, 8 blocks per set, 32 words per block, 8 bits per word
        C2: 2048 sets, 4 blocks per set, 32 words per block, 8 bits per word

    Contrast the difference between "data bit" capacity and "overhead bit" size
    for the two cacches. Assume that the word offset is log(M) low-order 
    (rightmost) bits, where M is the number of words per block; and that set 
    address is the middle log(N) low-order bits, where N is the number of sets. 
    The remaining (leftmost) bits are directory tags. Each set uses 32-bit 
    addresses.

 6.  Consider the 32-bit virtual address 

                   11110000 11110000 11110000 11110000

     for an L2 cache with 4 blocks per set, 2048 sets, and 128 words per block. 
     The address’s low-order bits are on the right. Assume standard formatting for a 
     cache address. (I’ve broken the address into  four chunks of 8 bits apiece 
     for readability only.)

       1.  Mark the bits that give the word offset in the line.
       2.  Mark the bits that specify the set.
       3.  Mark the bits that constitute the tag or key.
       4.  How many tags are in the directory as a whole?
       5.  How many directory tags must be compared against this address’s tag bits 
           when a cache lookup occurs?

 7. Some cache designers are abandoning LRU as replacement policy and going instead
    to random replacement. To see the problem, consider a direct-mapped cache 
    with one block that holds 6 words. Now consider this reference string for a
    code segment in a loop:

      1, 4, 7, 8, 9, 11, 13, 1, 4, 7, 8, 9, 11, 13, 1, 4, 7, 8, 9, 11, 13,...

    What problems does LRU cause given this reference string and our cache
    capacity? (Assume that words enter the cache in the order in which they
    arrive, for example, the word at address 1 goes into the first slot, the
    word at address 4 goes into the second slot, etc.) 

    Make the case that random replacement would yield better performance than
    does LRU for this reference string.
