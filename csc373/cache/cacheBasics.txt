Section 6.3 in text


Typical caches:

CPU registers:        4-byte words (0 latency)
TLB:                  address translations (32-bits per address) (0 latency)
L1 cache:             32-byte blocks (0-2 latency)

L2 cache:             32-byte blocks (2 - 10 latency)

Virtual memory:       4-KB page (100 latency in RAM)
Buffer cache:         Parts of files (100 latency in RAM) 

Network buffer cache: Parts of files (disk)        (10,000,000 latency = 10M)
Browser cache:        Web pages (disk, client)
Web cache:            Web pages (disk, remote server) (1,000,000,000 = 1G)
 

L1 and L2 issues: expensive SRAM and more transitors per bit stored

Program Locality: temporal (reusing pre-cached objects) and spatial (cost of
                  fetching a block amortized across subsequent accesses to that
                  block).

Cache Misses

   -- cold or compulsory miss: cache is empty WRT process P so that P's
      instruction/data must be loaded into the cache

   -- conflict miss: blocks from memory level K + 1 (e.g., RAM) 
      map to blocks at memory level K (e.g., L1), where sizeof(K) < sizeof(K + 1).
      For example, RAM blocks 0, 4, 8, and 12 map to cache block 0.
      Now suppose that process P requires blocks in this order:
 
               0,8,0,8,...,08

      RAM block 0 and 8 both map to cache block 0; hence, the loading of
      each RAM block causes a subsequent "conflict miss"

   -- capacity miss: a process typically uses a "working set" of cache
      blocks (e.g., blocks b1, b2, b3, and b4). But the cache cannot hold
      the entire working set; that is, sizeof(working set) > sizeof(allocated cache).

Cache structure:

  Cache is partitioned into sets. If just one set (fully associative), if more than
  one set (set-associative).

  N = number of sets

  A cache set is divided into lines, the analogs of memory pages. Each line includes
  a block of bytes. Each set has the same number of blocks.

  K = lines per set

  Each line has a certain number of words (bytes).

  L = bytes per line

     Example:   N = 256
                K = 4
                L = 64

                Capacity = N * K * L = 32,768 bytes

  The larger K, the more overhead bits == the larger the directory

  Definitions:

     N = 1           ==> fully associative
     N > 1 and K = 1 ==> direct mapped
     N > 1 and K > 1 ==> K-way set-associative

  In modern designs, TLB is often fully associative, L1s tend to be K-way set-associative,
  and L2 tend to be either set-associative or direct-mapped.
  

  "Generic" is set-associative:

     Multiple sets, each set consists of multiple lines:
     Each line:
               +------+-----------+--------------------------------------+
               | vbit | tag = key |  block of bytes                      |
               +------+-----------+--------------------------------------+

     The m address bits in the cache address are partitioned into

            -- set index bits
            -- tag bits
            -- block offset

     The vbit + tag bits = "overhead bits"
     The block of bytes  = "data bits"

     Cache capacity ignores overhead bits, e.g., a 256K cache means 256K of blocks

Definitions:

  The collection of all tag bits is called the cache directory.

  ** Direct-Mapped Cache: 1 line per set  (simplest to design/implement)
     
     Address format for DM cache:

                  +-----+-----------+-------------+
                  | tag | set index | byte offset |  
                  +-----+-----------+-------------+
                          middle bits so that contiguous blocks in memory
                          don't map to the same cache block

     Conceptually, a direct-mapped cache is just an array with a simple set index


  ** Set-Associative Cache: > 1 lines per set (E)
                            4, 8,...,128,256
             
     Replacement policy: (doesn't arise in DM) LRU, LFU, or random
     
     Conceptually, a set-associative cache is an associative memory (key/value pairs)

  ** Fully Associative: 1 set 

     Each line's tag must be searched in fully associative.
     Used only in TLBs, which tend to be small.

  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

  Definitions: A cache is "write through" if a write to the cache also writes
               back to main memory. 

               If a write to the cache does not automatically trigger a write
               to RAM, then the write to RAM occurs only when required (e.g.,
               another processes needs to access the RAM location); at this
               point, the cache contents are written back.

  Write issues: write through versus write back (dirty bit)
                no-write-allocate ==> write only to RAM, not to the cache
                    
                write-through are typically no-write-allocate
                write-back are typically write-allocate

  i-cache, d-cache, unified cache

  Typically, L1 has an i-cache and a d-cache
             L2 has a unified cache

  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

  Metrics:

      -- miss rate
      -- hit rate
      -- hit time
      -- miss penalty

  Cache design and metrics:

      -- the larger the cache, the higher the hit rate
      -- the larger the cache, the higher the hit time (more searching)

      -- the larger the block size, the more potential for locality
      -- the larger the block size, the smaller either the sets or the lines per set
 
      -- the higher the associativity, the lower the conflict misses
      -- the higher the associativity, the higher the ratio of tag bits to data bits

     
  Classic space/time tradeoff: Enhancing the hit rate may require large
                               associativity, that is, large directory.


  ****************************

  Cache addressing: most processors use virtual addresses but some (e.g., the Alpha)
                    use physical addresses


  Example with a 32-bit virtual address:

     512 sets, 4 lines per set, and 128 bytes per line

     log(512)   =  9 bits for the set
     log(128)   =  7 bits for the word offset
     32 - 9 - 7 = 16 bits for the tag

        16 bits          9 bits   7 bits
     +----------------+---------+-------+
     |   tag          |  set    | byte  |
     +----------------+---------+-------+      

  
  Example of DM versus K-way SA:

     C1 is K-way set associative: 512 sets, 8 lines per set, 2 words per line
                                  (Assume a 32-bit word)

     C1's capacity is 512 * 8 * 32 = 262,144 bits
        -- 9 bits for the set ID
        -- 1 bit for the word offset
        -- 22 bits for the tag

        -- directory size = 512 * 8 * 22 = 90,112 bits (34% of all bits)

     
     C2 is direct mapped: 4,096 sets, 1 line per set, 2 words per line
                          (Again assume a 32-bit word)

     C2's capacity i9s 4096 * 1 * 32 = 262,144 bits
        -- 12 bits for the set ID 
        -- 1 bit for the word offset
        -- 19 bits for the tag

        -- directory size = 4096 * 19 = 77,824 (30% of all bits)
